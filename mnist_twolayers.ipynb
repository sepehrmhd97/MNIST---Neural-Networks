{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "%matplotlib inline\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\n\nimport matplotlib.pyplot as plt\n\n# use GPU for computations if possible\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# temporarily patch this script until the MNIST data set download issue is resolved\n# https://github.com/pytorch/vision/issues/1938\nimport urllib.request\nopener = urllib.request.build_opener()\nopener.addheaders = [('User-agent', 'Mozilla/5.0')]\nurllib.request.install_opener(opener)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "collapsed": true, "id": "2ZfaQIUpDZvO", "tags": []}, "source": "# Classification of hand-written digits\n\nWe start by downloading and extracting the MNIST data set."}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false, "tags": []}, "outputs": [], "source": "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n                                      download=True, transform=transforms.ToTensor())\n\ntestset = torchvision.datasets.MNIST(root='./data', train=False,\n                                     download=True, transform=transforms.ToTensor())\n\n# extract a complete PyTorch dataset\ndef extract(dataset):\n    datasize = len(dataset)\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=datasize, shuffle=False)\n    return next(iter(dataloader))\n\n# extract all test images and labels into PyTorch tensors\n# the training data will be loaded in batches during training\ntest_X, test_Y = extract(testset)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "JetLsyAjDZv3", "tags": []}, "source": "## The model\n\nThe input data $X$ are grayscale images of $28\\times 28$ pixels. The first dimension will be the number of data points that are provided to the network. The input data is flattend into a matrix with $28 \\times 28 = 784$ columns using `X.view(-1, 784)`, where each colum represents one pixel. We then apply first the linear transformation $X W + b$ and then the softmax function to obtain the class probabilities predicted by the model."}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        # the weights of dimension (784, 10)\n        self.W = nn.Parameter(torch.zeros(784, 10))\n        # the offset vector of dimension (10,)\n        self.b = nn.Parameter(torch.zeros(10))\n\n    def forward(self, X):\n        # flatten the data into a matrix with 28 x 28 = 784 columns\n        X = X.view(-1, 784)\n        # compute the linear transformation\n        Z = X.mm(self.W) + self.b\n        # apply the softmax function\n        G = F.softmax(Z, dim=1)\n        return G"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "NYMZmV_GDZwa", "tags": []}, "source": "## The training\n\nWe define the cross-entropy for the predicted probabilities $G$ (10-dimensional vectors) and the labels $Y$ (integers between 0 and 9)."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "cOpD7LKcDZwe", "tags": []}, "outputs": [], "source": "def crossentropy(G, Y):\n    # convert labels to onehot encoding\n    Y_onehot = torch.eye(10, device=device)[Y]\n\n    return -(Y_onehot * G.log()).sum(dim = 1).mean()"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "vSpeO0byDZwx", "tags": []}, "source": "The next lines evaluate the accuracy of the predictions."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "G1B5xLOnDZw0", "tags": []}, "outputs": [], "source": "def accuracy(G, Y):\n    return (G.argmax(dim=1) == Y).float().mean()"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "collapsed": true, "id": "rJ4jX9FDDZxB", "tags": []}, "source": "We are ready to train the network."}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false, "tags": []}, "outputs": [], "source": "# initialize the test and training error statistics\ntest_accuracy = []\ntest_crossentropy = []\ntest_iter = []\ntrain_accuracy = []\ntrain_crossentropy = []\ntrain_iter = []\n\n# initialize the neural network and move it to the GPU if needed\nnet = Net()\nnet.to(device)\n\n# define the optimization algorithm\nlearningrate = 0.5\noptimizer = optim.SGD(net.parameters(), lr=learningrate)\n\n# define the data loader for batches of the training data\nbatchsize = 100\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batchsize, num_workers=2, shuffle=True)\n\n# perform multiple training steps\ntotal_iterations = 2000 # total number of iterations\nt = 0 # current iteration\ndone = False\nwhile not done:\n    for (batch_X, batch_Y) in trainloader:\n        # move batch to the GPU if needed\n        batch_X, batch_Y = batch_X.to(device), batch_Y.to(device)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward pass\n        batch_G = net(batch_X)\n        loss = crossentropy(batch_G, batch_Y)\n\n        # backpropagation\n        loss.backward()\n        \n        # perform gradient descent step\n        optimizer.step()\n        \n        # don't bother too much about the following lines!\n        with torch.no_grad():\n            # evaluate the performance on the training data at every 10th iteration\n            if t % 10 == 0:\n                train_crossentropy.append(loss.item())\n                train_accuracy.append(accuracy(batch_G, batch_Y).item())\n                train_iter.append(t)\n                \n            # evaluate the performance on the test data at every 100th iteration\n            if t % 100 == 0:\n                # move test data to the GPU if needed\n                X, Y = test_X.to(device), test_Y.to(device)\n\n                # compute predictions for the test data\n                G = net(X)\n                test_crossentropy.append(crossentropy(G, Y).item())\n                test_accuracy.append(accuracy(G, Y).item())\n                test_iter.append(t)\n\n                # print the iteration number and the accuracy of the predictions\n                print(f\"Step {t:5d}: train accuracy {100 * train_accuracy[-1]:6.2f}% \" \\\n                      f\"train cross-entropy {train_crossentropy[-1]:5.2f}  \" \\\n                      f\"test accuracy {100 * test_accuracy[-1]:6.2f}% \" \\\n                      f\"test cross-entropy {test_crossentropy[-1]:5.2f}\")\n            \n        # stop the training after the specified number of iterations\n        t += 1\n        if t > total_iterations:\n            done = True\n            break"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "wY9MEi2xDZxM", "tags": []}, "source": "## The evaluation\n\nThe remaining code produces the plots needed to evaluate the training and predictions."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 735}, "colab_type": "code", "id": "L_MSLJvoD28u", "outputId": "2be25bd3-0568-4590-faa2-39de35251d61", "tags": []}, "outputs": [], "source": "# plot the cross-entropy\nplt.plot(train_iter, train_crossentropy, 'b-', label='Training data (mini-batch)')\nplt.plot(test_iter, test_crossentropy, 'r-', label='Test data')\nplt.xlabel('Iteration')\nplt.ylabel('Cross-entropy')\nplt.ylim([0, min(test_crossentropy) * 3])\nplt.title('Cross-entropy')\nplt.grid(True)\nplt.legend(loc='best')\nplt.show()\n\n# plot the accuracy\nplt.plot(train_iter, train_accuracy, 'b-', label='Training data (mini-batch)')\nplt.plot(test_iter, test_accuracy, 'r-', label='Test data')\nplt.xlabel('Iteration')\nplt.ylabel('Prediction accuracy')\nplt.ylim([max(1 - (1 - test_accuracy[-1]) * 2, 0), 1])\nplt.title('Prediction accuracy')\nplt.grid(True)\nplt.legend(loc='best')\nplt.show()"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 575}, "colab_type": "code", "id": "caWJ8PwaDZxO", "outputId": "1d3e738d-e278-425a-ddb7-bcda6a4565f4", "scrolled": false, "tags": []}, "outputs": [], "source": "# evaluate the network on 100 random test images\nwith torch.no_grad():\n    # obtain 100 random samples from the test data set\n    random_X, random_Y = next(iter(torch.utils.data.DataLoader(testset, batch_size=100, shuffle=True)))\n    \n    # move data to the GPU if needed\n    random_X, random_Y = random_X.to(device), random_Y.to(device)\n    \n    # compute the predictions for the sampled inputs\n    random_G = net(random_X)\n    random_Yhat = random_G.argmax(dim=1)\n\n    # sort the predictions with the incorrect ones first\n    indices_incorrect_first = (random_Yhat == random_Y).float().argsort()\n\n# plot the images\nnum_rows = 10\nnum_cols = 10\nnum_images = num_rows * num_cols\nplt.figure(figsize=(num_cols, num_rows))\n\nfor i, index in enumerate(indices_incorrect_first, 1):\n    plt.subplot(num_rows, num_cols, i)\n    plt.xticks([])\n    plt.yticks([])\n    \n    # plot the image\n    plt.imshow(random_X[index, :, :].view(28, 28).cpu().numpy(), cmap=plt.cm.binary)\n    \n    # add the prediction as annotation (incorrect predictions in red, correct ones in blue)\n    color = 'blue' if random_Yhat[index] == random_Y[index] else 'red'\n    plt.text(0, 25, random_Yhat[index].item(), fontsize=25, color=color)\n        \nplt.show()"}], "metadata": {"@webio": {"lastCommId": null, "lastKernelId": null}, "anaconda-cloud": {}, "celltoolbar": "Tags", "colab": {"name": "mnist_one_layer.ipynb", "provenance": [], "version": "0.3.2"}, "kernelspec": {"display_name": "lab-sml", "language": "python", "name": "lab-sml"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.7"}}, "nbformat": 4, "nbformat_minor": 1}